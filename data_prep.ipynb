{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb8fc1d-5550-46a2-9961-702ae869aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import glob\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ae47a3-9ed7-468f-8ebe-444d22e84363",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523e49f4-fc29-478c-859d-3d8dabf2ace8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 16264\n"
     ]
    }
   ],
   "source": [
    "all_images_folder_path = os.path.join(\"data\", \"dfg\", \"images\")\n",
    "all_images_paths = glob.glob(os.path.join(all_images_folder_path, \"*.jpg\"))\n",
    "\n",
    "print(\"Number of images:\", len(all_images_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89740274-24a5-449c-a208-9bcf9e904a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dfg/train.json\") as f:\n",
    "    train_json = json.load(f)\n",
    "\n",
    "with open(\"data/dfg/test.json\") as f:\n",
    "    val_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef5fb12-a0c2-410a-848f-40380fd2e1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all names: 16264\n",
      "Number of available train names: 14029\n",
      "Number of available val names: 1703\n",
      "Total number of available names: 15732\n"
     ]
    }
   ],
   "source": [
    "all_filenames = set(map(lambda x: x.split(\"\\\\\")[-1], all_images_paths))\n",
    "train_filenames = set([i['file_name'] for i in train_json['images']])\n",
    "val_filenames = set([i['file_name'] for i in val_json['images']])\n",
    "\n",
    "print(\"Number of all names:\", len(all_filenames))\n",
    "print(\"Number of available train names:\", len(train_filenames))\n",
    "print(\"Number of available val names:\", len(val_filenames))\n",
    "\n",
    "print(\"Total number of available names:\", len((train_filenames | val_filenames) & all_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed54b9d4-3275-473e-aeb2-2bc2da003444",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf2208dd-15c2-44c4-ac46-67316338ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def prepare_set_annotations_dict(annotation_json, include_segmentation=False):\n",
    "    \"\"\"\n",
    "    Prepares a dictionary of annotations for each image ID from a list of annotation data.\n",
    "    \n",
    "    This function iterates over a list of annotation dictionaries, optionally excludes the 'segmentation'\n",
    "    data from each annotation, and organizes the annotations by their associated 'image_id'. Each 'image_id'\n",
    "    key in the resulting dictionary maps to a list of annotation dictionaries for that image.\n",
    "    \n",
    "    Parameters:\n",
    "    - annotation_json: A list of dictionaries, where each dictionary contains annotation data for an image.\n",
    "      The dictionary must include an 'image_id' key to associate the annotation with an image.\n",
    "    - include_segmentation: A boolean indicating whether to include 'segmentation' data in the annotations.\n",
    "      If False, the 'segmentation' field is removed from each annotation dictionary. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary keyed by 'image_id', where each value is a list of dictionaries containing annotation\n",
    "      data for the corresponding image. If 'include_segmentation' is False, the 'segmentation' field is\n",
    "      omitted from these dictionaries.\n",
    "    \"\"\"\n",
    "    annotations_dict = {}\n",
    "    \n",
    "    for ann in annotation_json:\n",
    "        ann_no_image_id = copy.copy(ann)\n",
    "        ann_no_image_id.pop('image_id')\n",
    "        if not include_segmentation:\n",
    "            ann_no_image_id.pop('segmentation', None)\n",
    "        \n",
    "        if ann['image_id'] not in annotations_dict:\n",
    "            annotations_dict[ann['image_id']] = [ann_no_image_id]\n",
    "        else:\n",
    "            annotations_dict[ann['image_id']].append(ann_no_image_id)\n",
    "    \n",
    "    return annotations_dict\n",
    "\n",
    "def add_images_to_master_dict(image_data_list, set_type, master_dict):\n",
    "    \"\"\"\n",
    "    Adds images to the specified set type in the master dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_data_list: List of dictionaries containing image data.\n",
    "    - set_type: The set type to add the images to ('train' or 'val').\n",
    "    - master_dict: The master dictionary where the images will be added.\n",
    "    \"\"\"\n",
    "    for image_data in image_data_list:\n",
    "        master_dict[set_type][image_data['id']] = image_data\n",
    "        master_dict[set_type][image_data['id']]['annotations'] = None\n",
    "\n",
    "def update_annotations(annotations_dict, set_type, master_dict):\n",
    "    \"\"\"\n",
    "    Updates the annotations for the specified set type in the master dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - annotations_dict: Dictionary containing annotations, keyed by image ID.\n",
    "    - set_type: The set type to update annotations for ('train' or 'val').\n",
    "    - master_dict: The master dictionary to update.\n",
    "    \"\"\"\n",
    "    for im_id, annotation in annotations_dict.items():\n",
    "        if im_id in master_dict[set_type]:\n",
    "            master_dict[set_type][im_id]['annotations'] = annotation\n",
    "\n",
    "def remove_images_without_annotations(set_type, master_dict):\n",
    "    \"\"\"\n",
    "    Removes images without annotations from the specified set type in the master dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - set_type: The set type to remove images from ('train' or 'val').\n",
    "    - master_dict: The master dictionary to update.\n",
    "    \"\"\"\n",
    "    im_ids_to_delete = [im_id for im_id, data in master_dict[set_type].items() if data['annotations'] is None]\n",
    "    for im_id in im_ids_to_delete:\n",
    "        del master_dict[set_type][im_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7c1bac-e959-4781-8ce8-3bf17940829b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train labels: 13970\n",
      "Number of val labels: 1703\n"
     ]
    }
   ],
   "source": [
    "train_annotations = prepare_set_annotations_dict(train_json['annotations'])\n",
    "val_annotations = prepare_set_annotations_dict(val_json['annotations'])\n",
    "        \n",
    "master_dict = {'train': {}, 'val': {}}\n",
    "\n",
    "# Populate master_dict with train and validation images\n",
    "add_images_to_master_dict(train_json['images'], 'train', master_dict)\n",
    "add_images_to_master_dict(val_json['images'], 'val', master_dict)\n",
    "\n",
    "# Update annotations in master_dict\n",
    "update_annotations(train_annotations, 'train', master_dict)\n",
    "update_annotations(val_annotations, 'val', master_dict)\n",
    "\n",
    "# Remove images without annotations\n",
    "remove_images_without_annotations('train', master_dict)\n",
    "remove_images_without_annotations('val', master_dict)\n",
    "\n",
    "print(\"Number of train labels:\", len(list(master_dict['train'].keys())))\n",
    "print(\"Number of val labels:\", len(list(master_dict['val'].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a2b2c7-4458-4da7-ab8d-4349c45c4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dfg/master.json\", \"w\") as f:\n",
    "    json.dump(master_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc2ba8-decd-4fb6-bfaa-caf260c4192c",
   "metadata": {},
   "source": [
    "### Save and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801da7f4-a55a-4720-92f4-a9803a2f9b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_json['categories'] == val_json['categories'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fe8fb92-b078-4a61-876c-61a6486eb417",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_text = \"\"\"\n",
    "path: dfg/\n",
    "train: 'train/images'\n",
    "val: 'val/images'\n",
    " \n",
    "# class names\n",
    "names: \n",
    "\"\"\"\n",
    "for i in train_json['categories']:\n",
    "    yaml_text += f\"  {i['id']}: {i['name']}\\n\"\n",
    "\n",
    "yaml_text = yaml_text.strip()\n",
    "\n",
    "\n",
    "with open(\"dfg_tsd.yaml\", \"w\") as f:\n",
    "    f.write(yaml_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84db89cc-466c-426f-9a76-29143176f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"datasets\\\\dfg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a5998df-67a8-40a3-a37d-6e2d8d241660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_label_text(image_info):\n",
    "    \"\"\"\n",
    "    Prepares and returns a text string containing the formatted annotations for a single image.\n",
    "    \n",
    "    The function iterates over the 'annotations' list in the provided image_info dictionary. Each annotation\n",
    "    includes a 'category_id' and a 'bbox' (bounding box), which is processed to calculate the center coordinates\n",
    "    (cx, cy), and the width and height (bw, bh) relative to the image dimensions. Annotations marked with 'ignore'\n",
    "    are skipped. The output is a string where each line corresponds to an annotation formatted as:\n",
    "    \"category_id center_x center_y bbox_width bbox_height\", with values normalized relative to the image size.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_info: A dictionary containing keys 'annotations', 'width', and 'height'. 'annotations' is a list of\n",
    "      dictionaries, each representing an object annotation with keys 'ignore', 'category_id', and 'bbox'. 'width'\n",
    "      and 'height' are the dimensions of the image.\n",
    "    \n",
    "    Returns:\n",
    "    - A string where each line represents an object annotation in the specified format. The coordinates and\n",
    "      dimensions are normalized by the image's width and height and rounded to five decimal places.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    for obj in image_info['annotations']:\n",
    "        \n",
    "        if obj.get('ignore', False):\n",
    "            continue\n",
    "            \n",
    "        ci = obj['category_id']\n",
    "        cx = np.clip((obj['bbox'][0] + obj['bbox'][2] / 2) / image_info['width'], 0, 1)\n",
    "        cy = np.clip((obj['bbox'][1] + obj['bbox'][3] / 2) / image_info['height'], 0, 1)\n",
    "        bw = np.clip(obj['bbox'][2] / image_info['width'], 0, 1)\n",
    "        bh = np.clip(obj['bbox'][3] / image_info['height'], 0, 1)\n",
    "        \n",
    "        text += f\"{ci} {cx:.5f} {cy:.5f} {bw:.5f} {bh:.5f}\\n\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a6086ac-448d-4576-8a3e-5caded53b030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'78 0.18958 0.40648 0.05417 0.12963\\n158 0.18542 0.30833 0.05417 0.07037\\n136 0.02865 0.28519 0.02604 0.06296\\n179 0.18411 0.24444 0.05260 0.06111\\n59 0.18255 0.15000 0.05260 0.13333'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_info = master_dict['train'][0]\n",
    "prepare_label_text(image_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d2b32ae-d9c2-4ef8-bea9-de82d9db4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def process_image_and_label(set_type, image_info, all_images_folder_path, datasets_path):\n",
    "    \"\"\"\n",
    "    Processes a single image and its corresponding label by saving the image in PNG format and\n",
    "    the label in TXT format.\n",
    "\n",
    "    Parameters:\n",
    "    - set_type: 'train' or 'val' indicating the dataset type.\n",
    "    - image_info: Dictionary containing the image information.\n",
    "    - all_images_folder_path: Path to the folder containing all images.\n",
    "    - datasets_path: Path to the base dataset folder where processed images and labels will be saved.\n",
    "    \"\"\"\n",
    "    # Save image\n",
    "    image = Image.open(os.path.join(all_images_folder_path, image_info['file_name']))\n",
    "    image_path = os.path.join(datasets_path, set_type, 'images', image_info['file_name'].replace(\".jpg\", \".png\"))\n",
    "    image.save(image_path)\n",
    "    image.close()\n",
    "\n",
    "    # Save label\n",
    "    label = prepare_label_text(image_info)  # Assuming prepare_label_text is defined elsewhere\n",
    "    label_path = os.path.join(datasets_path, set_type, 'labels', image_info['file_name'].replace(\".jpg\", \".txt\"))\n",
    "    with open(label_path, \"w\") as f:\n",
    "        f.write(label)\n",
    "\n",
    "def process_set(master_dict, set_type, all_images_folder_path, datasets_path, max_workers=100):\n",
    "    \"\"\"\n",
    "    Processes and saves images and labels for the specified dataset set_type using multithreading.\n",
    "\n",
    "    Parameters:\n",
    "    - master_dict: Master dictionary containing 'train' and 'val' keys with their respective image data.\n",
    "    - set_type: 'train' or 'val' indicating the dataset type to process.\n",
    "    - all_images_folder_path: Path to the folder containing all images.\n",
    "    - datasets_path: Path to the base dataset folder where processed images and labels will be saved.\n",
    "    - max_workers: Maximum number of threads to use. Default is 100.\n",
    "    \"\"\"\n",
    "    set_master = master_dict[set_type]\n",
    "    futures = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for im_id in list(set_master.keys()):  # Adjust as needed\n",
    "            image_info = set_master[im_id]\n",
    "            futures.append(executor.submit(process_image_and_label, set_type, image_info, all_images_folder_path, datasets_path))\n",
    "\n",
    "        for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures), position=0):\n",
    "            future.result()  # You can handle exceptions here if you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc601b78-bc00-4f78-9acc-0e097cca9173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13970/13970 [17:51<00:00, 13.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1703/1703 [03:11<00:00,  8.90it/s]\n"
     ]
    }
   ],
   "source": [
    "process_set(master_dict, 'train', all_images_folder_path, datasets_path, max_workers=500)\n",
    "process_set(master_dict, 'val', all_images_folder_path, datasets_path, max_workers=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
